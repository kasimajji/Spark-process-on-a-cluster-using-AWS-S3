# Apache-Spark-process-on-a-cluster-using-AWS-S3
In this project, building robust ELT in a data hosted on Amazon S3. This pipeline should extract raw data hosted in Amazon S3, transform it using Apache Spark into structured and optimized analytics tables, and then load the processed data back into S3 for further use.This involves cleaning, aggregating, and organizing the data into an appropriate structure so that analytics and reporting can be performed on the data. The ELT workflow shall be deployed to an AWS-hosted, scalable Spark cluster for high-performance execution and efficient handling of large datasets. With this solution, decision-making becomes truly data-driven, and preparation of data for analytics purposes becomes seamless and faster.
